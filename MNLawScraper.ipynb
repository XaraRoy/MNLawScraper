{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f11c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                        ## Dataframe manipulation\n",
    "import requests                          ##\n",
    "from requests_html import HTMLSession  ## We dont have to manipulate the webpage at all, so a chrome based scraper is not needed\n",
    "from bs4 import BeautifulSoup as bs  ## how we read the info from the request\n",
    "import csv                         ## how we will store the data portably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchDF(df, column, query):\n",
    "    searchDF = df[df[column].apply(str).str.contains(query, na=False)]\n",
    "    return searchDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readBill(billUrl):\n",
    "            ##   This currently only gets the most recent text of the bill.\n",
    "            ##  This page has much more info, particularly the bill revisions\n",
    "    htmlRequester = HTMLSession()\n",
    "    r = htmlRequester.get('http://' + billUrl)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    billcard = soup.find_all(class_=\"card-body\")\n",
    "    billtextUrl = 'http://revisor.mn.gov/bills/' + billcard[0].find('a')['href'].strip('/bills')\n",
    "    r = htmlRequester.get(billtextUrl)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    billText = soup.find(id='document').text.replace('\\n', '')\n",
    "    return billText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumeScrape(finalDF):\n",
    "    urlsToCheck = len(searchDF(finalDF, 'scrapeComplete', 'False')['billUrl'].unique())\n",
    "    for count, url in enumerate(searchDF(finalDF, 'scrapeComplete', 'False')['billUrl'].unique()):\n",
    "        indexes = finalDF.loc[finalDF['billUrl'] == url].index\n",
    "        indexes\n",
    "        finalDF.at[indexes[0], 'text'] = readBill(url)\n",
    "        finalDF.at[indexes[0], 'scrapeComplete'] = 'True'\n",
    "\n",
    "        for index in indexes[1:]:\n",
    "            finalDF.at[index, 'text'] = 'duplicate'\n",
    "            finalDF.at[index, 'scrapeComplete'] = 'True'\n",
    "            \n",
    "        \n",
    "        if count % 5 == 0:\n",
    "            print(count, ' of ', urlsToCheck)\n",
    "        \n",
    "        if count >  5:     ##  DELETE THESE LINES TO UNLIMIT DEMO\n",
    "            return finalDF  ##   CURRENTLY ONLY CHECKS 5 Bills a Session\n",
    "                             ##   Limiting Legislature, Session Below as well (search limit)\n",
    "\n",
    "                               ## Maybe should write line out to csv instead of holding in DF?\n",
    "    return finalDF\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'mnLaws.csv'\n",
    "try:\n",
    "    dataframe = pd.read_csv(fileName)\n",
    "    finalDF = resumeScrape(dataFrame)\n",
    "    finalDF.to_csv(fileName)\n",
    "except :\n",
    "    with open(fileName, 'a', newline='') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=' ')\n",
    "        header = ['LegislatureName', 'LegislatureUrl', 'sessionYear', 'sessionType',\n",
    "       'sessionUrl', 'chapter', 'chapterUrl', 'bill', 'billUrl', 'text',\n",
    "       'PresentmentDate', 'scrapeComplete']\n",
    "        ## The DOM breaks it down into bill sections and subdivisions,\n",
    "        ##but we just need the text for todays project\n",
    "        csvWriter.writerow(header)\n",
    "        print('new file created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff95463",
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlRequester = HTMLSession()\n",
    "# r = requests.get('https://www.revisor.mn.gov/laws/')  ## if just using requests, not requestsHTML, maybe we can test the speed\n",
    "r = htmlRequester.get('https://www.revisor.mn.gov/laws/')  ## or efficency of these three methods (requests, html, chromium)\n",
    "soup = bs(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring the data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289db7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d62fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatures = soup.find_all('tr','alternate')  ## Using the inspect tool here to find the identifier for the data we want\n",
    "legislatures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatures[0].find('a', href=True)['href']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa8e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatures[0].find_all('td')[1].text.strip('\\n ').rstrip('\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionTracker = pd.DataFrame(data=None, index=None, columns= ['LegislatureName', 'LegislatureUrl'])\n",
    "print(sessionTracker)\n",
    "\n",
    "\n",
    "yearlysessionTracker = pd.DataFrame(data=None, index=None, columns= ['LegislatureName', 'sessionYear', 'sessionType', 'sessionUrl'])\n",
    "print(yearlysessionTracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLLECTING LIST OF LEGISLATURES  (I didnt turn these into functions because each page is too different)\n",
    "\n",
    "for session in legislatures:\n",
    "    name = session.find_all('td')[1].text.strip('\\n ').rstrip('\\n ')\n",
    "    url = session.find('a', href=True)['href'].strip('//')\n",
    "    sessionTracker.loc[len(sessionTracker.index)] = [name, url]\n",
    "sessionTracker.head()\n",
    "      ## TODO We are missing the 92nd legislature // Most recent posting, it has a different layout, manualy added \n",
    "newRow = pd.DataFrame({'LegislatureName':'92nd Legislature', 'LegislatureUrl':'www.revisor.mn.gov/laws/92.0'},index=[0])\n",
    "sessionTracker = pd.concat([newRow, sessionTracker]).reset_index(drop = True)\n",
    "sessionTracker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting List of Sessions in a Legislature\n",
    "\n",
    "iterator = 0        \n",
    "while iterator < 5:  ## Limit 5\n",
    "# while iterator < len(sessionTracker.index):\n",
    "    sessionName = sessionTracker.loc[iterator]['LegislatureName']\n",
    "    url = sessionTracker.loc[iterator]['LegislatureUrl']\n",
    "\n",
    "    r = htmlRequester.get('http://' + url)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    sessions = soup.find_all('p', 'p_session')\n",
    "    for session in sessions:\n",
    "        yearlySession = session.find_all('a')[0].text.replace('\\n', '').strip(' ').rstrip(' ')\n",
    "\n",
    "        sessionYear = yearlySession[0:4]\n",
    "        sessionType = yearlySession[6:].strip(' ')\n",
    "        sessionUrl = sessions[0].find_all('a', href=True)[0]['href'].strip('//')\n",
    "        yearlysessionTracker.loc[len(yearlysessionTracker.index)] = [sessionName, sessionYear, sessionType, sessionUrl]\n",
    "    iterator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlysessionTracker.head(5)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5996085",
   "metadata": {},
   "outputs": [],
   "source": [
    "billTracker = pd.DataFrame(data=None, index=None, columns= (['sessionUrl', 'chapter',  'chapterUrl', 'bill', 'billUrl', 'text', 'PresentmentDate', 'scrapeComplete']))\n",
    "billTracker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496253dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collecting List Bills in  a session\n",
    "iterator = 0 \n",
    "while iterator < 5: ## Limit 5\n",
    "# while iterator < len(yearlysessionTracker.index):\n",
    "\n",
    "    sessionUrl = yearlysessionTracker.loc[iterator]['sessionUrl']\n",
    "    r = htmlRequester.get('http://' + sessionUrl)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    chapters = soup.find_all('tr')\n",
    "    try:\n",
    "        for chapter in chapters[1:]:\n",
    "            chapterUrl = chapter.find_all('a',href=True)[0]['href'].strip('//')\n",
    "            chapterName =chapter.find_all('a',href=True)[0].text[7:]\n",
    "            BillUrl = 'revisor.mn.gov/' + chapter.find_all('a',href=True)[1]['href'].strip('//')\n",
    "            BillName = chapter.find_all('a',href=True)[1].text\n",
    "            PresentmentDate = chapter.find_all('td')[2].text\n",
    "            text = \"\"\n",
    "            billTracker.loc[len(billTracker.index)] = [sessionUrl, chapterName, chapterUrl, BillName, BillUrl, text, PresentmentDate, False]\n",
    "    except:\n",
    "        billTracker.loc[len(billTracker.index)] = [sessionUrl, chapterName, chapterUrl, BillName, BillUrl, text, PresentmentDate, 'ERROR']\n",
    "    iterator += 1\n",
    "## TODO ASK DOMAIN EXPERT ABOUT CHAPTERS,for now we just continue towards bill text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8704196",
   "metadata": {},
   "outputs": [],
   "source": [
    "billTracker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumeScrape(billTracker)  ## search 'limit' to find 3 limiters to run full scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDF(billTracker, 'scrapeComplete', 'Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We made it down to the bill page, which has so much good info on it.  \n",
    "## for today, we are just going to get the bill text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17095fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(billTracker.head(1)['text'][0])\n",
    "billTracker.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionTracker.merge(yearlysessionTracker.merge(billTracker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe59421",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionTracker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlysessionTracker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "billTracker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0778206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatureSessionCombo = sessionTracker.merge(yearlysessionTracker, how='outer')\n",
    "legislatureSessionCombo.head(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatureSessionCombo.tail(10)  ## At this point i realized that the territoral legislatures were broken somehow\n",
    "                                ## Examining the page showed a different style, fixing beyond scope of demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20140987",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF = legislatureSessionCombo.merge(billTracker, how='outer', on='sessionUrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.to_csv('mnLaws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06712bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDF(finalDF, 'text', 'transportation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39918ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDF(finalDF, 'text', 'eagle')  ## 92nd Legislature, 2021 1st Special Session\n",
    "                                    ## from searchbar on revisor.gov\n",
    "                                    ## couldnt find because query in CHAPTER text,\n",
    "                                    ## not bill text,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1366480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for reading! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
