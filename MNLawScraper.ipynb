{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c188ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  ## Dataframe manipulation\n",
    "import requests\n",
    "from requests_html import HTMLSession ## We dont have to manipulate the webpage at all, so a chrome based scraper is not needed\n",
    "from bs4 import BeautifulSoup as bs ## how we read the info from the request\n",
    "import csv ## how we will store the data portably\n",
    "import re ## this is for helping find the bill text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchDF(df, column, query):\n",
    "    searchDF = df[df[column].apply(str).str.contains(query, na=False)]\n",
    "\n",
    "    return searchDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3974ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for datafile in local folder, or create it\n",
    "fileName = 'mnLaws.csv'\n",
    "try:\n",
    "    dataframe = pd.read_csv(fileName)\n",
    "    dataframe.head()\n",
    "    \n",
    "    ##TODO ContinueScrape()  ####   Demo is set up as if starting from 0 #####\n",
    "except :\n",
    "    with open(fileName, 'w', newline='') as csvfile:\n",
    "        csvWriter = csv.writer(csvfile, delimiter=' ')\n",
    "        header = ['LegislatureName', 'LegislatureUrl', 'sessionYear', 'sessionType',\n",
    "       'sessionUrl', 'chapter', 'chapterUrl', 'bill', 'billUrl', 'text',\n",
    "       'PresentmentDate', 'scrapeComplete']\n",
    "        ## The DOM breaks it down into bill sections and subdivisions,\n",
    "        ##but we just need the text for todays project\n",
    "        csvWriter.writerow(header)\n",
    "        \n",
    "        dataframe = pd.read_csv(file)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlRequester = HTMLSession()\n",
    "# r = requests.get('https://www.revisor.mn.gov/laws/')  ## if just using requests, not requestsHTML, maybe we can test the speed\n",
    "r = htmlRequester.get('https://www.revisor.mn.gov/laws/')  ## or efficency of these three methods (requests, html, chromium)\n",
    "print(r.status_code, r.encoding)\n",
    "soup = bs(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df6934",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring the data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715fbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatures = soup.find_all('tr','alternate')  ## Using the inspect tool here to find the identifier for the data we want\n",
    "legislatures[0]\n",
    "\n",
    "## TODO the first row in the table does not have the alternate tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatures[0].find('a', href=True)['href']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatures[0].find_all('td')[1].text.strip('\\n ').rstrip('\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionTracker = pd.DataFrame(data=None, index=None, columns= ['LegislatureName', 'LegislatureUrl'])\n",
    "## using a df to track my progress on scraping, for a more polished project we should use another program to start these scrapes,\n",
    "## so we could use more computing power / threads, and for resiliencey of the scraper to unexpected page issues\n",
    "print(sessionTracker)\n",
    "\n",
    "\n",
    "yearlysessionTracker = pd.DataFrame(data=None, index=None, columns= ['LegislatureName', 'sessionYear', 'sessionType', 'sessionUrl'])\n",
    "print(yearlysessionTracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4159ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COLLECTING LIST OF LEGISLATURES  (I didnt turn these into functions because each page is too different)\n",
    "\n",
    "for session in legislatures:\n",
    "    name = session.find_all('td')[1].text.strip('\\n ').rstrip('\\n ')\n",
    "    url = session.find('a', href=True)['href'].strip('//')\n",
    "    sessionTracker.loc[len(sessionTracker.index)] = [name, url]\n",
    "sessionTracker.head()\n",
    "        ## TODO We are missing the 92nd legislature, it has a different layout, should manualy add to \n",
    "      ## session tracker or try to programaticly fix (only current year has this layout)? answer depends on use\n",
    "      ## if i was trying to just fill the data lake, instead of just analyzing bill text, more time could\n",
    "      ## be spent fixing?\n",
    "newRow = pd.DataFrame({'LegislatureName':'92nd Legislature', 'LegislatureUrl':'www.revisor.mn.gov/laws/92.0'},index=[0])\n",
    "sessionTracker = pd.concat([newRow, sessionTracker]).reset_index(drop = True)\n",
    "sessionTracker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting List of Sessions in a Legislature\n",
    "\n",
    "iterator = 0         ## to limit requests here replace <5 with < len(sessionTracker.index) for complete scrape\n",
    "while iterator < 15:\n",
    "# while iterator < len(sessionTracker.index):\n",
    "    sessionName = sessionTracker.loc[iterator]['LegislatureName']\n",
    "    url = sessionTracker.loc[iterator]['LegislatureUrl']\n",
    "\n",
    "    r = htmlRequester.get('http://' + url)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    sessions = soup.find_all('p', 'p_session')\n",
    "    for session in sessions:\n",
    "        yearlySession = session.find_all('a')[0].text.replace('\\n', '').strip(' ').rstrip(' ')\n",
    "\n",
    "        sessionYear = yearlySession[0:4]\n",
    "        sessionType = yearlySession[6:].strip(' ')\n",
    "        sessionUrl = sessions[0].find_all('a', href=True)[0]['href'].strip('//')\n",
    "        yearlysessionTracker.loc[len(yearlysessionTracker.index)] = [sessionName, sessionYear, sessionType, sessionUrl]\n",
    "    iterator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0310de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlysessionTracker.head(25\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd917243",
   "metadata": {},
   "outputs": [],
   "source": [
    "billTracker = pd.DataFrame(data=None, index=None, columns= (['sessionUrl', 'chapter',  'chapterUrl', 'bill', 'billUrl', 'text', 'PresentmentDate', 'scrapeComplete']))\n",
    "billTracker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd110e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting List Bills in  a session\n",
    "iterator = 0 ## to limit requests here replace <5 with < len(yearlysessionTracker.index) for complete scrape\n",
    "while iterator < 15:\n",
    "# while iterator < len(yearlysessionTracker.index):\n",
    "\n",
    "    sessionUrl = yearlysessionTracker.loc[iterator]['sessionUrl']\n",
    "    r = htmlRequester.get('http://' + sessionUrl)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    chapters = soup.find_all('tr')\n",
    "    try:\n",
    "        for chapter in chapters[1:]:\n",
    "            chapterUrl = chapter.find_all('a',href=True)[0]['href'].strip('//')\n",
    "            chapterName =chapter.find_all('a',href=True)[0].text[7:]\n",
    "            BillUrl = 'revisor.mn.gov/' + chapter.find_all('a',href=True)[1]['href'].strip('//')\n",
    "            BillName = chapter.find_all('a',href=True)[1].text\n",
    "            PresentmentDate = chapter.find_all('td')[2].text\n",
    "            text = \"\"\n",
    "            billTracker.loc[len(billTracker.index)] = [sessionUrl, chapterName, chapterUrl, BillName, BillUrl, text, PresentmentDate, False]\n",
    "    except:\n",
    "        billTracker.loc[len(billTracker.index)] = [sessionUrl, chapterName, chapterUrl, BillName, BillUrl, text, PresentmentDate, 'ERROR']\n",
    "    iterator += 1\n",
    "## TODO ASK DOMAIN EXPERT ABOUT CHAPTERS,for now we just continue towards bill text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb6edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDF(billTracker, 'scrapeComplete', 'Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93872d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "billTracker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally we made it down to the bill page, which has so much good info on it.  \n",
    "## for today, we are just going to get the bill text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0 ## to limit requests here replace <5 with < len(billTracker.index) for complete scrape\n",
    "while iterator < 5:\n",
    "# while iterator < len(billTracker.index):\n",
    "    billUrl = billTracker.loc[iterator]['billUrl']\n",
    "    print(billUrl)\n",
    "    \n",
    "### \n",
    "    r = htmlRequester.get('http://' + billUrl)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    billcard = soup.find_all(class_=\"card-body\")\n",
    "    billtextUrl = 'http://revisor.mn.gov/bills/' + billcard[0].find('a')['href'].strip('/bills')\n",
    "    print(billtextUrl)\n",
    "\n",
    "    r = htmlRequester.get(billtextUrl)\n",
    "    soup = bs(r.text, 'html.parser')\n",
    "    billText = soup.find(id='document').text.replace('\\n', '')\n",
    "    billTracker.iat[iterator, 5] = billText\n",
    "    billTracker.iat[iterator, -1] = True\n",
    "\n",
    "###\n",
    "    \n",
    "    \n",
    "    iterator += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(billTracker.head(1)['text'][0])\n",
    "billTracker.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34bc8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionTracker.merge(yearlysessionTracker.merge(billTracker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionTracker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da123487",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlysessionTracker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "billTracker.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatureSessionCombo = sessionTracker.merge(yearlysessionTracker, how='outer')\n",
    "legislatureSessionCombo.head(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatureSessionCombo.tail(10)  ## At this point i realized that the territoral legislatures were broken somehow\n",
    "# Examining the page showed a different style, fixing not needed at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF = legislatureSessionCombo.merge(billTracker, how='outer', on='sessionUrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.to_csv('mnLaws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16687d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDF(finalDF, 'text', 'transportation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchDF(finalDF, 'text', 'eagle')  ## 92nd Legislature, 2021 1st Special Session\n",
    "                                    ## from searchbar on revisor.gov\n",
    "                                    ## couldnt find because query in CHAPTER text,\n",
    "                                    ## not bill text, TODO WHAT IS CHAPTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for reading! This scraper is not totaly complete,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
